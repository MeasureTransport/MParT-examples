### A Pluto.jl notebook ###
# v0.19.9

using Markdown
using InteractiveUtils

# ╔═╡ faa4c23a-23bc-11ed-1303-ad77785ff8f1
# ╠═╡ show_logs = false
using Pkg; Pkg.add(url="https://github.com/MeasureTransport/MParT.jl")

# ╔═╡ faa4c2b0-23bc-11ed-033b-9564a75396aa
using MParT, Distributions, LinearAlgebra, Statistics, Optimization, OptimizationOptimJL, GLMakie

# ╔═╡ faa4c330-23bc-11ed-222e-0dbe64cf4348
md"""
# Transport map from samples
The objective of this example is to show how a transport map can be build in MParT when samples from the target density are known.
## Problem formulation
From the definition of a transport map, the *function* $S(\mathbf{x}; \mathbf{w})$ is invertible and have a positive definite Jacobian for any parameters $w$.  Combined with a probability density $\eta(\mathbf{r})$, we
can therefore define a density $\tilde{\pi}_w(x)$ induced by transforming $r$ with the inverse map $S^{-1}(\mathbf{r})$.   More precisely, the change of random variable formula from the reference $\eta$ to the target $\tilde{\pi}$ reads:

```math
\tilde{\pi}_{\mathbf{w}}(\mathbf{x}) = \eta(S(\mathbf{x}; \mathbf{w}))\left| \det\nabla S(\mathbf{x}; \mathbf{w})\right|,
```

where $\det\nabla S$ is the determinant of the map Jacobian at the point $\mathbf{x}$.   We refer to $\tilde{\pi}_{\mathbf{w}}(\mathbf{x})$ as the *map-induced* density or *pullback distribution* and will commonly interchange notation for densities and measures to use the notation $\tilde{\pi} = S^{\sharp} \eta$.

The objective of this example is, from samples $\mathbf{x}^i$, $i \in \{1,...,n\}$ drawn according to a density $\pi$, build the map-induced density approximation $\tilde{\pi}$.
"""

# ╔═╡ faa5a3f4-23bc-11ed-17a2-6b2055b31cac
md"""
## Imports
First, import MParT and other packages used in this notebook. Note that it is possible to specify the number of threads used by MParT by setting the `KOKKOS_NUM_THREADS` environment variable **before** importing MParT.
"""

# ╔═╡ faa5a432-23bc-11ed-34fa-e5a0f656e636
begin
import numpy as np
from scipy.optimize import minimize
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

import os
os.environ["KOKKOS_NUM_THREADS"] = "8"

import mpart as mt
print("Kokkos is using", Concurrency(), "threads")
rcParams["figure.dpi"] = 110
end

# ╔═╡ faa75d36-23bc-11ed-0524-337036e27956
begin
end

# ╔═╡ faa75d5e-23bc-11ed-1ffe-4b11bca6dbce
md"""
## Reference density and samples

In this example we use a 2D target density known as the *banana* density where the unnormalized probability density, samples and the exact transport map are known.

The banana density is defined as:
```math
\pi(x_1,x_2) \propto N_1(x_1)\times N_1(x_2-x_1^2)
```
where $N_1$ is the 1D standard normal density.

The exact transport map that transport $\pi$ to the 2D standard normal density is defined as:
```math
{S}^\text{true}(x_1,x_2)=
\begin{bmatrix}
x_1\\
x_2 - x_1^2
\end{bmatrix}
```
"""

# ╔═╡ faa75dea-23bc-11ed-15ab-436ff433dc84
md"""
Samples from $\pi$ are generated by the following:
"""

# ╔═╡ faa75df4-23bc-11ed-3345-1180447f40ce
begin
# Make target samples for training
num_points = 10000
r = random.randn(2,num_points)
x1 = r[0]
x2 = r[1] + r[0] .^2
x = vstack([x1,x2])


# Make target samples for testing
test_r = random.randn(2,5000)
test_x1 = test_r[0]
test_x2 = test_r[1] + test_r[0] .^2
test_x = vstack([test_x1,test_x2])
end

# ╔═╡ faa7a110-23bc-11ed-37a4-3d958ce8c9e7
begin
end

# ╔═╡ faa7a124-23bc-11ed-1d36-af70cedd453a
md"""
### Plot training samples:
"""

# ╔═╡ faa7a138-23bc-11ed-19af-81124f5ab682
begin
fig1 = Figure()
ax1 = Axis(fig1[1,1])
scatter(x[0,:],x[1,:], facecolor=(:blue,0.1), label="Target samples")
ax1.xlabel = "$x_1$"
ax1.ylabel = "$x_2$"
axislegend()
fig1
end

# ╔═╡ faa7bf2e-23bc-11ed-2525-c90aaa432471
md"""
## Map training

### Defining objective function and its gradient

To match the map induced density $\tilde{\pi}_{\mathbf{w}}(\mathbf{x})$ with the samples, we can maximize the likelihood of observing the samples, which is simply
```math
\prod_{i=1}^N \tilde{\pi}_w(\mathbf{x}^i).
```

Numerically, it is typically easier to work with the log-likelihood instead and we will therefore maximize the log likelihood to find the parameters $w$:

```math
w^\ast = \underset{\mathbf{w}}{\operatorname{argmax}} \sum_{i=1}^N \log \tilde{\pi}_{\mathbf{w}}(\mathbf{x}^i).
```

Importantly, our use of triangular maps and a standard normal reference density allows us to expand this objective into two independent problems: one for the parameters $w_1$ defining the first component $S_1(x_1; w_1)$ of the map, and one for the parameters defining the second component $S_2(x_{1:2}; w_2)$.
In general, for map component $k$, the objective function is given by

```math
J_k(\mathbf{w}_k) = - \frac{1}{N}\sum_{i=1}^N \left( \log\eta\left(S_k(\mathbf{x}_{1:k}^i;\mathbf{w}_k)\right) + \log \frac{\partial S_k(\mathbf{x}_{1:k}^i;\mathbf{w}_k)}{\partial x_k}\right)
```

and the resulting optimization problem is

```math
\mathbf{w}_k^\ast = \underset{\mathbf{w}_k}{\operatorname{argmin}} J_k(\mathbf{w}_k).
```

In order to use efficient gradient-based minimizer we need to define both the objective and its gradient.  The gradient is given by

```math
\nabla_{\mathbf{w}_k}J_k(\mathbf{w}_k) = - \frac{1}{N}\sum_{i=1}^N \left(\left[\nabla_{\mathbf{w}_k}S_k
(\mathbf{x}_{1:k}^i;\mathbf{w}_k)\right]^T \nabla_\mathbf{r}\log \eta \left(S_k
(\mathbf{x}_{1:k}^i;\mathbf{w}_k)\right) - \frac{\partial \nabla_{\mathbf{w}_k}S_k(\mathbf{x}_{1:k}^i;\mathbf{w}_k)}{\partial x_k} \left[\frac{\partial S_k(\mathbf{x}_{1:k}^i;\mathbf{w}_k)}{\partial x_k}\right]^{-1}\right),
```

where $\nabla_{\mathbf{w}_k}S_k(\mathbf{x}_{1:k}^i;\mathbf{w}_k)$ is the Jacobian of the map output with respect to the map parameters and $\nabla_\mathbf{r}\log \eta \left(S_k(\mathbf{x}_{1:k}^i;\mathbf{w}_k)\right)$ is the gradient of the reference log-density evaluated at the map output.  Note that for a standard normal reference density, this expression simplifies to $\nabla_\mathbf{r}\log \eta \left(S_k(\mathbf{x}_{1:k}^i;\mathbf{w}_k)\right) = -S_k(\mathbf{x}_{1:k}^i;\mathbf{w}_k)$.
"""

# ╔═╡ faa7c00a-23bc-11ed-0e36-0b27ccc80247
begin
# Reference density
rho1 = multivariate_normal(zeros(1),eye(1))

# Negative log likelihood objective
function obj(coeffs,p)
	tri_map,x = p
    """ Evaluates the log-likelihood of the samples using the map-induced density. """
    num_points = size(x,1)
    SetCoeffs(tri_map, coeffs)

    # Compute the map-induced density at each point
    map_of_x = Evaluate(tri_map, x)
    rho_of_map_of_x = rho1.logpdf(map_of_x.T)
    log_det = tri_map.LogDeterminant(x)

    # Return the negative log-likelihood of the entire dataset
    -sum(rho_of_map_of_x + log_det)/num_points
end

function grad_obj(coeffs,p)
	tri_map, x = p
    """ Returns the gradient of the log-likelihood objective wrt the map parameters. """
    num_points = size(x,1)
    SetCoeffs(tri_map, coeffs)

    # Evaluate the map
    map_of_x = Evaluate(tri_map, x)

    # Now compute the inner product of the map jacobian (\nabla_w S) and the gradient (which is just -S(x) here)
    grad_rho_of_map_of_x = CoeffGrad(-tri_map, x, map_of_x)

    # Get the gradient of the log determinant with respect to the map coefficients
    grad_log_det = tri_map.LogDeterminantCoeffGrad(x)

    -sum(grad_rho_of_map_of_x + grad_log_det, 1)/num_points
end


end

# ╔═╡ faa87444-23bc-11ed-1cd3-e9c280d3d95f
begin
end

# ╔═╡ faa8745a-23bc-11ed-3556-8f640037eedc
md"""
### Map parameterization

With the separability property of the objective function mentionned above we can parameterize and optimize components $S_1$ and $S_2$ independently.

"""

# ╔═╡ faa87478-23bc-11ed-26dc-47b444fa7240
md"""
#### First component $S_1$

Theoritically the first component is $S_1^{\text{true}}(x_1)=x_1$. This parameterization can be set with MParT as:
"""

# ╔═╡ faa87482-23bc-11ed-306c-a70b5c13dca8
begin
multis1 = array([[0],[1]]);
mset1= MultiIndexSet(multis1)
fixed_mset1 = Fix(mset1, True)
end

# ╔═╡ faa88378-23bc-11ed-0419-1b23de1b00d5
md"""
Let's define the first component:
"""

# ╔═╡ faa88396-23bc-11ed-2642-11a74021e38b
begin
# Set-up first component and initialize map coefficients
map_options1 = MapOptions()

# Create map component
S1 = CreateComponent(fixed_mset1,map_options1)
end

# ╔═╡ faa89ade-23bc-11ed-1855-61d63ecce75b
begin
end

# ╔═╡ faa89ae8-23bc-11ed-2f65-458b5bbe8ea6
md"""
#### Second component $S_2$

Theoritically the second component is $S_2^{\text{true}}(x_1,x_2)=x_2^2-x_1$. The corresponding multi-index set can exactly be define as:
"""

# ╔═╡ faa89b06-23bc-11ed-3dc9-61aa9dd1c0a6
begin
multis2_true = array([[0,1],[2,0]]);
mset2_true = MultiIndexSet(multis2_true)
fixed_mset2_true = Fix(mset2_true, True)
end

# ╔═╡ faa8aa10-23bc-11ed-389a-3f7c1147f1a6
md"""
Other multi-index sets which include the true multi-index set are any total order expansion of order greater than one:
"""

# ╔═╡ faa8aa30-23bc-11ed-16e3-dbfa7c5136b1
begin
total_order2 = 2
fixed_mset2 = FixedMultiIndexSet(2,total_order2)
end

# ╔═╡ faa8b3ca-23bc-11ed-12ad-236e8463326f
md"""
This parameterization will include terms: $x_1$, $x_2^2$, $x_1 x_2$ that are not required to approximate the true map.
"""

# ╔═╡ faa8b3de-23bc-11ed-2bfc-951bb24581bd
md"""
Let's define $S_2$ with one multi-index set.
"""

# ╔═╡ faa8b3e8-23bc-11ed-256f-97fa701194a1
begin
map_options2 = MapOptions()
S2 = CreateComponent(fixed_mset2,map_options2) #or S2 = CreateComponent(fixed_mset2_true,map_options2)
end

# ╔═╡ faa8bda2-23bc-11ed-2356-6da6e2f02e0b
md"""
### Approximation before optimization

Coefficients of map components are set to 0 upon creation. The triangular transport map composed by $S_1$ and $S_2$ is defined by:
"""

# ╔═╡ faa8bdb6-23bc-11ed-1d23-c38e0c557471
begin
transport_map = TriangularMap((S1,S2))
transport_map.SetCoeffs(concatenate((CoeffMap(S1),CoeffMap(S2))))
end

# ╔═╡ faa8c770-23bc-11ed-0f19-8ba491fe7487
md"""
Reference density:
"""

# ╔═╡ faa8c77a-23bc-11ed-2251-71ee42f2797d
md"""
For plotting and computing reference density
ef_distribution = multivariate_normal(np.zeros(2),np.eye(2))  #standard normal
= np.linspace(-5,5,100)
rid = np.meshgrid(t,t)
ef_pdf_at_grid = ref_distribution.pdf(np.dstack(grid))
"""

# ╔═╡ faa8c798-23bc-11ed-10d2-c1c27326233f
md"""
Transport of target samples via $S$:
"""

# ╔═╡ faa8c7a2-23bc-11ed-33f2-c1fa28ef986b
begin
r_test_before_opt = Evaluate(transport_map, test_x)
end

# ╔═╡ faa8cc90-23bc-11ed-1132-41b61e7e3f2a
begin
# Before optimization plot
fig2 = Figure()
ax2 = Axis(fig2[1,1])
ax2.title = "Before optimization"
contour(*grid, ref_pdf_at_grid)
scatter(r_test_before_opt[0,:],r_test_before_opt[1,:], facecolor=(:blue,0.1), label="Pushed target samples through $S$")
axislegend()
ax2.xlabel = "$r_1$"
ax2.ylabel = "$r_2$"
fig2


# Print initial coeffs and objective
print("==================")
print("Starting coeffs component 1:")
print(CoeffMap(S1))
print("Objective value for component 1: {:.2E}".format(obj(CoeffMap(S1), S1, test_x[:1,])))
print("==================")
print("Starting coeffs component 2:")
print(CoeffMap(S2))
print("Objective value for component 2: {.2E}".format(obj(CoeffMap(S2), S2, test_x)))
print("==================")
end

# ╔═╡ faa94646-23bc-11ed-0389-8bdcc4a65f45
begin
end

# ╔═╡ faa9465a-23bc-11ed-2506-39cf9f4d510e
md"""
### Optimization

Optimization of $S_1$ and $S_2$ coefficients are performed independently.
"""

# ╔═╡ faa94678-23bc-11ed-23b7-2fad3b85c1d0
md"""
#### Optimization of $S_1$

For $S_1$ only samples of the first coordinate $x_1$ are required to solve the minimization problem.
"""

# ╔═╡ faa9468c-23bc-11ed-1027-8fc5a44c80e3
md"""
Optimize
ptimizer_options={'gtol': 1e-5, 'disp': True}
es = minimize(obj, S1.CoeffMap(), args=(S1, x[:1,:]), jac=grad_obj, method='BFGS', options=optimizer_options)
"""

# ╔═╡ faa946a0-23bc-11ed-067a-adfee5e76195
md"""
#### Optimization of $S_2$
"""

# ╔═╡ faa946aa-23bc-11ed-2918-bdcd069eeb96
md"""
Optimize
ptimizer_options={'gtol': 1e-5, 'disp': True}
es = minimize(obj, S2.CoeffMap(), args=(S2, x), jac=grad_obj, method='BFGS', options=optimizer_options)
"""

# ╔═╡ faa946be-23bc-11ed-3c53-23d073dd22c3
md"""
### Approximation after optimization
"""

# ╔═╡ faa946ca-23bc-11ed-1559-29f0b79eaf76
md"""
#### Normality of pushed samples:


"""

# ╔═╡ faa946dc-23bc-11ed-3ae7-61bf0ae2fa9a
md"""
Building triangular map from components:
"""

# ╔═╡ faa946e6-23bc-11ed-2c4c-59db886c4579
begin
transport_map = TriangularMap((S1,S2))
transport_map.SetCoeffs(concatenate((CoeffMap(S1),CoeffMap(S2))))
end

# ╔═╡ faa950c8-23bc-11ed-13d7-3fa3794002b1
md"""
Transport of testing samples from target:
"""

# ╔═╡ faa950dc-23bc-11ed-1741-816894c08419
begin
r_test_after_opt = Evaluate(transport_map, test_x)
end

# ╔═╡ faa955b4-23bc-11ed-3075-e950fa5480c0
begin
# Before optimization plot
fig3 = Figure()
ax3 = Axis(fig3[1,1])
ax3.title = "After optimization"
contour(*grid, ref_pdf_at_grid)
scatter(r_test_after_opt[0,:],r_test_after_opt[1,:], facecolor=(:blue,0.1), label="Pushed target samples through $S$")
axislegend()
ax3.xlabel = "$r_1$"
ax3.ylabel = "$r_2$"
fig3


# Print final coeffs and objective
print("==================")
print("Final coeffs component 1:")
print(CoeffMap(S1))
print("Objective value for component 1: {:.2E}".format(obj(CoeffMap(S1), S1, test_x[:1,])))
print("==================")
print("Final coeffs component 2:")
print(CoeffMap(S2))
print("Objective value for component 2: {.2E}".format(obj(CoeffMap(S2), S2, test_x)))
print("==================")
end

# ╔═╡ faa9c164-23bc-11ed-34c5-a35b174a5780
begin
end

# ╔═╡ faa9c170-23bc-11ed-336f-2df055db36f4
md"""
After optimization testing samples are visually distributed according to the standard normal which tell us that the map has been computed accurately. Another estimation of the approximation quality is to test normality of the pushed samples. One simple way to do that is to compute first moments of the pushed test samples:
"""

# ╔═╡ faa9c184-23bc-11ed-0fd6-e197061f1a2c
md"""
Print statistics of normalized samples (TODO replace with better Gaussianity check)
rint('==================')
ean_of_map = np.mean(r_test_after_opt,1)
rint("Mean of normalized test samples")
rint(mean_of_map)
rint('==================')
rint("Cov of normalized test samples")
ov_of_map = np.cov(r_test_after_opt)
rint(cov_of_map)
rint('==================')
"""

# ╔═╡ faa9c1ac-23bc-11ed-33d3-094a6296326e
md"""
Here mean should be 0 and covariance matrix should be identity.
"""

# ╔═╡ faa9c1b6-23bc-11ed-3746-3921070ff606
md"""
#### Comparison with the true transport map
"""

# ╔═╡ faa9c1ca-23bc-11ed-1505-f9664f7fb098
md"""
Since the true transport map for this problem is known, we can compare directly map evaluations component by component.
"""

# ╔═╡ faa9c1d4-23bc-11ed-00a1-d987d48511f9
begin
# Evaluation grid

ngrid=100
x1_t = range(-3,3,ngrid)
x2_t = range(-3,7.5,ngrid)
xx1,xx2 = meshgrid(x1_t,x2_t)

xx = vstack((reshape(xx1, 1,-1),reshape(xx2, 1,-1)))
end

# ╔═╡ faa9e81c-23bc-11ed-2c9f-b3d1d57010e9
begin
end

# ╔═╡ faa9e826-23bc-11ed-3f69-6ff8114be570
md"""
##### First component
"""

# ╔═╡ faa9e83a-23bc-11ed-2975-87cd411e27de
begin
fig4 = Figure()
ax4 = Axis(fig4[1,1])
ax4.title = "$S_1(x_1)$"
lines!(ax4, x1_t,x1_t,label="true map")
scatter!(ax4, x1_t,x1_t,label="true map")
lines!(ax4, x1_t,Evaluate(S1, reshape(x1_t, 1,-1)).flatten(),"--",label="map approximation")
scatter!(ax4, x1_t,Evaluate(S1, reshape(x1_t, 1,-1)).flatten(),"--",label="map approximation")
ax4.xlabel = "$x_1$"
axislegend()
fig4
end

# ╔═╡ faaa0cc8-23bc-11ed-2591-83e1cf90c08b
md"""
##### Second component
"""

# ╔═╡ faaa0ce8-23bc-11ed-0e56-910af2de00f3
begin
map_eval_true =  xx[1,:] - xx[0,:] .^2
map_eval_approx = Evaluate(S2, xx)

fig5 = Figure()
ax5 = Axis(fig5[1,1])
ax5.title = "$S_2(x_1,x_2)$"
contour(*grid, reshape(map_eval_true, 100,100))
contour(*grid, reshape(map_eval_approx, 100,100),linestyles="--")
ax5.xlabel = "$x_1$"
ax5.ylabel = "$x_2$"
fig5


end

# ╔═╡ faaa4726-23bc-11ed-21e7-53159d7fbe1b
begin
end

# ╔═╡ faaa473a-23bc-11ed-1e5a-6d978ded191f
md"""
#### Contours of map-induced density

We can also compare contours of the map-induced density and true unnormalized density.
"""

# ╔═╡ faaa4750-23bc-11ed-170e-d36b7aa2ce91
begin
# Map induced pdf
function pullback_pdf(tri_map,rho,x)
    r = Evaluate(tri_map, x)
    log_pdf = rho.logpdf(r.T)+tri_map.LogDeterminant(x)
    exp(log_pdf)
end

# True density
function target_logpdf(x)
  rv1 = multivariate_normal(zeros(1),eye(1))
  rv2 = multivariate_normal(zeros(1),eye(1))
  logpdf1 = rv1.logpdf(x[0])
  logpdf2 = rv2.logpdf(x[1]-x[0] .^2)
  logpdf = logpdf1 + logpdf2
  logpdf
end


# +
# Comparison grid

ngrid=100
x1_t = range(-3,3,ngrid)
x2_t = range(-3,7.5,ngrid)
xx1,xx2 = meshgrid(x1_t,x2_t)

xx = vstack((reshape(xx1, 1,-1),reshape(xx2, 1,-1)))

# For plotting and computing densities

true_pdf_at_grid = exp(target_logpdf(xx))

map_induced_pdf = pullback_pdf(transport_map,ref_distribution,xx)

fig, ax = subplots()
#SCA = ax.scatter(test_x[0,:2000],test_x[1,:2000], facecolor=(:blue,0.1),label="Target samples")
CS1 = ax.contour(xx1, xx2, reshape(true_pdf_at_grid, ngrid,ngrid))
CS2 = ax.contour(xx1, xx2, reshape(map_induced_pdf, ngrid,ngrid),linestyles="--")
ax.set_ax5.xlabel = r"$x_1$"
ax.set_ax5.ylabel = r"$x_2$"
h1,_ = CS1.axislegend_elements()
h2,_ = CS2.axislegend_elements()
axislegend1 = ax.axislegend([h1[0], h2[0]], ["Unnormalized target", "TM approximation"])
fig5

# ╔═╡ Cell order:
# ╠═faa4c23a-23bc-11ed-1303-ad77785ff8f1
# ╠═faa4c2b0-23bc-11ed-033b-9564a75396aa
# ╠═faa4c330-23bc-11ed-222e-0dbe64cf4348
# ╠═faa5a3f4-23bc-11ed-17a2-6b2055b31cac
# ╠═faa5a432-23bc-11ed-34fa-e5a0f656e636
# ╠═faa75d36-23bc-11ed-0524-337036e27956
# ╠═faa75d5e-23bc-11ed-1ffe-4b11bca6dbce
# ╠═faa75dea-23bc-11ed-15ab-436ff433dc84
# ╠═faa75df4-23bc-11ed-3345-1180447f40ce
# ╠═faa7a110-23bc-11ed-37a4-3d958ce8c9e7
# ╠═faa7a124-23bc-11ed-1d36-af70cedd453a
# ╠═faa7a138-23bc-11ed-19af-81124f5ab682
# ╠═faa7bf2e-23bc-11ed-2525-c90aaa432471
# ╠═faa7c00a-23bc-11ed-0e36-0b27ccc80247
# ╠═faa87444-23bc-11ed-1cd3-e9c280d3d95f
# ╠═faa8745a-23bc-11ed-3556-8f640037eedc
# ╠═faa87478-23bc-11ed-26dc-47b444fa7240
# ╠═faa87482-23bc-11ed-306c-a70b5c13dca8
# ╠═faa88378-23bc-11ed-0419-1b23de1b00d5
# ╠═faa88396-23bc-11ed-2642-11a74021e38b
# ╠═faa89ade-23bc-11ed-1855-61d63ecce75b
# ╠═faa89ae8-23bc-11ed-2f65-458b5bbe8ea6
# ╠═faa89b06-23bc-11ed-3dc9-61aa9dd1c0a6
# ╠═faa8aa10-23bc-11ed-389a-3f7c1147f1a6
# ╠═faa8aa30-23bc-11ed-16e3-dbfa7c5136b1
# ╠═faa8b3ca-23bc-11ed-12ad-236e8463326f
# ╠═faa8b3de-23bc-11ed-2bfc-951bb24581bd
# ╠═faa8b3e8-23bc-11ed-256f-97fa701194a1
# ╠═faa8bda2-23bc-11ed-2356-6da6e2f02e0b
# ╠═faa8bdb6-23bc-11ed-1d23-c38e0c557471
# ╠═faa8c770-23bc-11ed-0f19-8ba491fe7487
# ╠═faa8c77a-23bc-11ed-2251-71ee42f2797d
# ╠═faa8c798-23bc-11ed-10d2-c1c27326233f
# ╠═faa8c7a2-23bc-11ed-33f2-c1fa28ef986b
# ╠═faa8cc90-23bc-11ed-1132-41b61e7e3f2a
# ╠═faa94646-23bc-11ed-0389-8bdcc4a65f45
# ╠═faa9465a-23bc-11ed-2506-39cf9f4d510e
# ╠═faa94678-23bc-11ed-23b7-2fad3b85c1d0
# ╠═faa9468c-23bc-11ed-1027-8fc5a44c80e3
# ╠═faa946a0-23bc-11ed-067a-adfee5e76195
# ╠═faa946aa-23bc-11ed-2918-bdcd069eeb96
# ╠═faa946be-23bc-11ed-3c53-23d073dd22c3
# ╠═faa946ca-23bc-11ed-1559-29f0b79eaf76
# ╠═faa946dc-23bc-11ed-3ae7-61bf0ae2fa9a
# ╠═faa946e6-23bc-11ed-2c4c-59db886c4579
# ╠═faa950c8-23bc-11ed-13d7-3fa3794002b1
# ╠═faa950dc-23bc-11ed-1741-816894c08419
# ╠═faa955b4-23bc-11ed-3075-e950fa5480c0
# ╠═faa9c164-23bc-11ed-34c5-a35b174a5780
# ╠═faa9c170-23bc-11ed-336f-2df055db36f4
# ╠═faa9c184-23bc-11ed-0fd6-e197061f1a2c
# ╠═faa9c1ac-23bc-11ed-33d3-094a6296326e
# ╠═faa9c1b6-23bc-11ed-3746-3921070ff606
# ╠═faa9c1ca-23bc-11ed-1505-f9664f7fb098
# ╠═faa9c1d4-23bc-11ed-00a1-d987d48511f9
# ╠═faa9e81c-23bc-11ed-2c9f-b3d1d57010e9
# ╠═faa9e826-23bc-11ed-3f69-6ff8114be570
# ╠═faa9e83a-23bc-11ed-2975-87cd411e27de
# ╠═faaa0cc8-23bc-11ed-2591-83e1cf90c08b
# ╠═faaa0ce8-23bc-11ed-0e56-910af2de00f3
# ╠═faaa4726-23bc-11ed-21e7-53159d7fbe1b
# ╠═faaa473a-23bc-11ed-1e5a-6d978ded191f
# ╠═faaa4750-23bc-11ed-170e-d36b7aa2ce91
